wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id o533ihjc.
wandb: Tracking run with wandb version 0.13.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Logging to ./temp/models/tsp_diffusion/o533ihjc
Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
----------------------------------------------------------------------------------------------------
GNNEncoder(
  (node_embed): Linear(in_features=256, out_features=256, bias=True)
  (edge_embed): Linear(in_features=256, out_features=256, bias=True)
  (pos_embed): PositionEmbeddingSine()
  (edge_pos_embed): ScalarEmbeddingSine()
  (time_embed): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
  )
  (out): Sequential(
    (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
    (1): ReLU()
    (2): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  )
  (layers): ModuleList(
    (0): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (6): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (7): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (8): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (9): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (10): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (11): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (time_embed_layers): ModuleList(
    (0): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (1): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (2): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (3): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (4): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (5): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (6): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (7): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (8): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (9): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (10): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (11): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
  )
  (per_layer_out): ModuleList(
    (0): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (1): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (2): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (3): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (4): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (5): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (6): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (7): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (8): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (9): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (10): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (11): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
----------------------------------------------------------------------------------------------------

Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3
Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 3 processes
----------------------------------------------------------------------------------------------------

Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1,2,3]
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/nn/parallel/distributed.py:1754: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
Parameters: 5333762
Training steps: 10410

  | Name  | Type       | Params
-------------------------------------
0 | model | GNNEncoder | 5.3 M 
-------------------------------------
5.3 M     Trainable params
0         Non-trainable params
5.3 M     Total params
10.668    Total estimated model params size (MB)
Froze bottom 8 GNN layers for fine-tuning
Built L2SP anchor snapshot for 140 params.
Sanity Checking: 0it [00:00, ?it/s]Validation dataset size: 64
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]                                                                           /data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:241: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  category=PossibleUserWarning,
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val/2opt_iterations', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.
  f"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to"
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/1063 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1063 [00:00<?, ?it/s] Validation dataset size: 64
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/nn/parallel/distributed.py:1754: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
Validation dataset size: 64
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/nn/parallel/distributed.py:1754: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
Epoch 0:   2%|▏         | 20/1063 [03:14<2:49:12,  9.73s/it]Epoch 0:   2%|▏         | 20/1063 [03:14<2:49:12,  9.73s/it, loss=57.1, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.810]Epoch 0:   4%|▍         | 40/1063 [06:22<2:43:14,  9.57s/it, loss=57.1, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.810]Epoch 0:   4%|▍         | 40/1063 [06:22<2:43:14,  9.57s/it, loss=53.3, v_num=ihjc, train/pref_pairs=8.000, train/infer_cost=7.830]Epoch 0:   6%|▌         | 60/1063 [09:07<2:32:24,  9.12s/it, loss=53.3, v_num=ihjc, train/pref_pairs=8.000, train/infer_cost=7.830]Epoch 0:   6%|▌         | 60/1063 [09:07<2:32:24,  9.12s/it, loss=46.1, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.800]Epoch 0:   8%|▊         | 80/1063 [11:58<2:27:04,  8.98s/it, loss=46.1, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.800]Epoch 0:   8%|▊         | 80/1063 [11:58<2:27:04,  8.98s/it, loss=44, v_num=ihjc, train/pref_pairs=10.00, train/infer_cost=7.810]  Epoch 0:   9%|▉         | 100/1063 [14:48<2:22:40,  8.89s/it, loss=44, v_num=ihjc, train/pref_pairs=10.00, train/infer_cost=7.810]Epoch 0:   9%|▉         | 100/1063 [14:48<2:22:40,  8.89s/it, loss=43.8, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.740]Epoch 0:  11%|█▏        | 120/1063 [17:31<2:17:39,  8.76s/it, loss=43.8, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.740]Epoch 0:  11%|█▏        | 120/1063 [17:31<2:17:39,  8.76s/it, loss=45.6, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.820]Epoch 0:  13%|█▎        | 140/1063 [20:19<2:13:58,  8.71s/it, loss=45.6, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.820]Epoch 0:  13%|█▎        | 140/1063 [20:19<2:13:58,  8.71s/it, loss=51.5, v_num=ihjc, train/pref_pairs=8.000, train/infer_cost=7.840]Epoch 0:  15%|█▌        | 160/1063 [23:03<2:10:08,  8.65s/it, loss=51.5, v_num=ihjc, train/pref_pairs=8.000, train/infer_cost=7.840]Epoch 0:  15%|█▌        | 160/1063 [23:03<2:10:08,  8.65s/it, loss=44.2, v_num=ihjc, train/pref_pairs=10.00, train/infer_cost=7.800]Epoch 0:  17%|█▋        | 180/1063 [25:49<2:06:43,  8.61s/it, loss=44.2, v_num=ihjc, train/pref_pairs=10.00, train/infer_cost=7.800]Epoch 0:  17%|█▋        | 180/1063 [25:49<2:06:43,  8.61s/it, loss=49.2, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.820]Epoch 0:  19%|█▉        | 200/1063 [28:36<2:03:24,  8.58s/it, loss=49.2, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.820]Epoch 0:  19%|█▉        | 200/1063 [28:36<2:03:24,  8.58s/it, loss=37.2, v_num=ihjc, train/pref_pairs=10.00, train/infer_cost=7.780]Epoch 0:  21%|██        | 220/1063 [31:16<1:59:50,  8.53s/it, loss=37.2, v_num=ihjc, train/pref_pairs=10.00, train/infer_cost=7.780]Epoch 0:  21%|██        | 220/1063 [31:16<1:59:50,  8.53s/it, loss=45, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.820]  Epoch 0:  23%|██▎       | 240/1063 [34:01<1:56:39,  8.51s/it, loss=45, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.820]Epoch 0:  23%|██▎       | 240/1063 [34:01<1:56:39,  8.51s/it, loss=39.4, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.790]Epoch 0:  24%|██▍       | 260/1063 [36:43<1:53:25,  8.48s/it, loss=39.4, v_num=ihjc, train/pref_pairs=9.000, train/infer_cost=7.790]Epoch 0:  24%|██▍       | 260/1063 [36:43<1:53:25,  8.48s/it, loss=28.8, v_num=ihjc, train/pref_pairs=13.00, train/infer_cost=7.770]Epoch 0:  26%|██▋       | 280/1063 [39:23<1:50:08,  8.44s/it, loss=28.8, v_num=ihjc, train/pref_pairs=13.00, train/infer_cost=7.770]Epoch 0:  26%|██▋       | 280/1063 [39:23<1:50:08,  8.44s/it, loss=38.7, v_num=ihjc, train/pref_pairs=17.00, train/infer_cost=7.800]Epoch 0:  28%|██▊       | 300/1063 [42:02<1:46:54,  8.41s/it, loss=38.7, v_num=ihjc, train/pref_pairs=17.00, train/infer_cost=7.800]Epoch 0:  28%|██▊       | 300/1063 [42:02<1:46:54,  8.41s/it, loss=50.5, v_num=ihjc, train/pref_pairs=6.000, train/infer_cost=7.770]Epoch 0:  30%|███       | 320/1063 [44:39<1:43:42,  8.37s/it, loss=50.5, v_num=ihjc, train/pref_pairs=6.000, train/infer_cost=7.770]Epoch 0:  30%|███       | 320/1063 [44:39<1:43:42,  8.37s/it, loss=41.4, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.810]Epoch 0:  32%|███▏      | 340/1063 [47:17<1:40:34,  8.35s/it, loss=41.4, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.810]Epoch 0:  32%|███▏      | 340/1063 [47:17<1:40:34,  8.35s/it, loss=33.4, v_num=ihjc, train/pref_pairs=11.00, train/infer_cost=7.780]Epoch 0:  34%|███▍      | 360/1063 [49:56<1:37:30,  8.32s/it, loss=33.4, v_num=ihjc, train/pref_pairs=11.00, train/infer_cost=7.780]Epoch 0:  34%|███▍      | 360/1063 [49:56<1:37:30,  8.32s/it, loss=34.8, v_num=ihjc, train/pref_pairs=8.000, train/infer_cost=7.820]Epoch 0:  36%|███▌      | 380/1063 [52:35<1:34:31,  8.30s/it, loss=34.8, v_num=ihjc, train/pref_pairs=8.000, train/infer_cost=7.820]Epoch 0:  36%|███▌      | 380/1063 [52:35<1:34:31,  8.30s/it, loss=36.4, v_num=ihjc, train/pref_pairs=11.00, train/infer_cost=7.830]Epoch 0:  38%|███▊      | 400/1063 [55:15<1:31:36,  8.29s/it, loss=36.4, v_num=ihjc, train/pref_pairs=11.00, train/infer_cost=7.830]Epoch 0:  38%|███▊      | 400/1063 [55:15<1:31:36,  8.29s/it, loss=31.3, v_num=ihjc, train/pref_pairs=15.00, train/infer_cost=7.820]Epoch 0:  40%|███▉      | 420/1063 [57:56<1:28:42,  8.28s/it, loss=31.3, v_num=ihjc, train/pref_pairs=15.00, train/infer_cost=7.820]Epoch 0:  40%|███▉      | 420/1063 [57:56<1:28:42,  8.28s/it, loss=29.4, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.830]Epoch 0:  41%|████▏     | 440/1063 [1:00:38<1:25:51,  8.27s/it, loss=29.4, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.830]Epoch 0:  41%|████▏     | 440/1063 [1:00:38<1:25:51,  8.27s/it, loss=35.6, v_num=ihjc, train/pref_pairs=15.00, train/infer_cost=7.780]Epoch 0:  43%|████▎     | 460/1063 [1:03:20<1:23:01,  8.26s/it, loss=35.6, v_num=ihjc, train/pref_pairs=15.00, train/infer_cost=7.780]Epoch 0:  43%|████▎     | 460/1063 [1:03:20<1:23:01,  8.26s/it, loss=31.3, v_num=ihjc, train/pref_pairs=15.00, train/infer_cost=7.870]Epoch 0:  45%|████▌     | 480/1063 [1:06:02<1:20:12,  8.26s/it, loss=31.3, v_num=ihjc, train/pref_pairs=15.00, train/infer_cost=7.870]Epoch 0:  45%|████▌     | 480/1063 [1:06:02<1:20:12,  8.26s/it, loss=26.7, v_num=ihjc, train/pref_pairs=18.00, train/infer_cost=7.890]Epoch 0:  47%|████▋     | 500/1063 [1:08:45<1:17:24,  8.25s/it, loss=26.7, v_num=ihjc, train/pref_pairs=18.00, train/infer_cost=7.890]Epoch 0:  47%|████▋     | 500/1063 [1:08:45<1:17:24,  8.25s/it, loss=25.3, v_num=ihjc, train/pref_pairs=11.00, train/infer_cost=7.860]Epoch 0:  49%|████▉     | 520/1063 [1:11:28<1:14:37,  8.25s/it, loss=25.3, v_num=ihjc, train/pref_pairs=11.00, train/infer_cost=7.860]Epoch 0:  49%|████▉     | 520/1063 [1:11:28<1:14:37,  8.25s/it, loss=22, v_num=ihjc, train/pref_pairs=15.00, train/infer_cost=7.950]  Epoch 0:  51%|█████     | 540/1063 [1:14:11<1:11:51,  8.24s/it, loss=22, v_num=ihjc, train/pref_pairs=15.00, train/infer_cost=7.950]Epoch 0:  51%|█████     | 540/1063 [1:14:11<1:11:51,  8.24s/it, loss=22.3, v_num=ihjc, train/pref_pairs=18.00, train/infer_cost=7.950]Epoch 0:  53%|█████▎    | 560/1063 [1:16:54<1:09:05,  8.24s/it, loss=22.3, v_num=ihjc, train/pref_pairs=18.00, train/infer_cost=7.950]Epoch 0:  53%|█████▎    | 560/1063 [1:16:54<1:09:05,  8.24s/it, loss=19.1, v_num=ihjc, train/pref_pairs=14.00, train/infer_cost=7.970]