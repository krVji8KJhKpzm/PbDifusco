wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id ui06qokv.
wandb: Tracking run with wandb version 0.13.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Logging to ./temp/models/tsp_diffusion/ui06qokv
Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
----------------------------------------------------------------------------------------------------
GNNEncoder(
  (node_embed): Linear(in_features=256, out_features=256, bias=True)
  (edge_embed): Linear(in_features=256, out_features=256, bias=True)
  (pos_embed): PositionEmbeddingSine()
  (edge_pos_embed): ScalarEmbeddingSine()
  (time_embed): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
  )
  (out): Sequential(
    (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
    (1): ReLU()
    (2): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  )
  (layers): ModuleList(
    (0): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (6): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (7): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (8): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (9): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (10): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (11): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (time_embed_layers): ModuleList(
    (0): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (1): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (2): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (3): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (4): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (5): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (6): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (7): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (8): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (9): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (10): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (11): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
  )
  (per_layer_out): ModuleList(
    (0): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (1): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (2): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (3): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (4): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (5): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (6): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (7): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (8): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (9): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (10): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (11): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
----------------------------------------------------------------------------------------------------

Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3
Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 3 processes
----------------------------------------------------------------------------------------------------

Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_train.txt" with 100000 lines
Loaded "./temp/data/tsp/tsp100_auto_test.txt" with 1000 lines
Loaded "./temp/data/tsp/tsp100_auto_val.txt" with 1000 lines
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1,2,3]
Parameters: 5333762
Training steps: 10410

  | Name  | Type       | Params
-------------------------------------
0 | model | GNNEncoder | 5.3 M 
-------------------------------------
5.3 M     Trainable params
0         Non-trainable params
5.3 M     Total params
21.335    Total estimated model params size (MB)
Froze bottom 6 GNN layers for fine-tuning
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [1,2,3]
Built L2SP anchor snapshot for 168 params.
Sanity Checking: 0it [00:00, ?it/s]Validation dataset size: 64
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.30it/s]                                                                           /data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:241: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  category=PossibleUserWarning,
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val/2opt_iterations', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.
  f"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to"
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/1063 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1063 [00:00<?, ?it/s] Validation dataset size: 64
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Validation dataset size: 64
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Traceback (most recent call last):
  File "/data1/gushengda/PbDifusco/difusco/train.py", line 241, in <module>
    main(args)
  File "/data1/gushengda/PbDifusco/difusco/train.py", line 225, in main
    trainer.fit(model)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 697, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1283, in _run_train
    self.fit_loop.run()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 271, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1550, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1705, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/ddp.py", line 289, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 216, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 153, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 138, in _wrap_closure
    closure_result = closure()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 141, in closure
    self._backward_fn(step_output.closure_loss)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 304, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1704, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 191, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, optimizer, optimizer_idx, *args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, optimizer_idx, *args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1450, in backward
    loss.backward(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 146, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
Parameter at index 251 has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration. You can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print parameter names for further debugging.
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Traceback (most recent call last):
  File "difusco/train.py", line 241, in <module>
    main(args)
  File "difusco/train.py", line 225, in main
    trainer.fit(model)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 697, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 648, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1283, in _run_train
    self.fit_loop.run()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 271, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1550, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1705, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/ddp.py", line 289, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 216, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 153, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 138, in _wrap_closure
    closure_result = closure()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 141, in closure
    self._backward_fn(step_output.closure_loss)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 304, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1704, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 191, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, optimizer, optimizer_idx, *args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, optimizer_idx, *args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1450, in backward
    loss.backward(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 146, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
Parameter at index 251 has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration. You can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print parameter names for further debugging.
wandb: Waiting for W&B process to finish... (failed 1).
/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Traceback (most recent call last):
  File "/data1/gushengda/PbDifusco/difusco/train.py", line 241, in <module>
    main(args)
  File "/data1/gushengda/PbDifusco/difusco/train.py", line 225, in main
    trainer.fit(model)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 697, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1283, in _run_train
    self.fit_loop.run()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 271, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1550, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1705, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/ddp.py", line 289, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 216, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 153, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 138, in _wrap_closure
    closure_result = closure()
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 141, in closure
    self._backward_fn(step_output.closure_loss)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 304, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1704, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 191, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, optimizer, optimizer_idx, *args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, optimizer_idx, *args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1450, in backward
    loss.backward(*args, **kwargs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 146, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/data1/anaconda3/envs/difusco/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
Parameter at index 251 has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration. You can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print parameter names for further debugging.
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: You can sync this run to the cloud by running:
wandb: wandb sync ./temp/models/wandb/offline-run-20251031_073420-ui06qokv
wandb: Find logs at: ./temp/models/wandb/offline-run-20251031_073420-ui06qokv/logs
